{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTLFtgKJfI/tHMgRG63rax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LAXMINARAYAN-SHARMA/INTRADAY_DATA/blob/main/my_daily_intraday_updater_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"intraday_data_fetch_upstox_api_final.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1yLR0KrTo_o3-VUjodgtvOrB_K--3Lf5K\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "# df=pd.read_csv('/content/ind_nifty50list.csv')\n",
        "df=pd.read_csv('complete.csv')\n",
        "# df_eq=pd.read_csv('/content/EQUITY_L (1).csv')\n",
        "df_eq_500=pd.read_csv('/content/ind_nifty500list.csv')\n",
        "# df_eq_500=df_eq_500.iloc[:5,:]\n",
        "names_to_search = [row['Symbol'] for index, row in df_eq_500.iterrows()]\n",
        "names_to_search=names_to_search[:1]\n",
        "names_to_search\n",
        "# df_eq_500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G60Dk3VQRg0",
        "outputId": "26a8a040-5784-4cab-a9ac-eaf6a47fd441"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['360ONE']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"historical_data_in_simple_table_for_n_years.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1RJSU82yAjVUV06cGK7IchzBvXZ3Bk1yc\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to fetch intraday 1-minute data\n",
        "def fetch_intraday_data(symbol):\n",
        "    url = f'https://api.upstox.com/v2/historical-candle/intraday/{symbol}/1minute'\n",
        "    headers = {'Accept': 'application/json'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json().get('data', [])\n",
        "        return pd.DataFrame(data)\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code} - {response.text}\")\n",
        "        return None\n",
        "\n",
        "# # Define the symbols for which you want to fetch intraday data\n",
        "# symbols = ['NSE_EQ%7CINE848E01016']\n",
        "\n",
        "# Function to fetch instrument_keys based on a list of names using boolean indexing\n",
        "def get_instrument_keys(df, names):\n",
        "    instrument_keys = {}\n",
        "\n",
        "    for name in names:\n",
        "        mask = df['tradingsymbol'] == name\n",
        "        if mask.any():\n",
        "            instrument_key = df.loc[mask, 'instrument_key'].iloc[0]\n",
        "            instrument_keys[name.lower()] = instrument_key\n",
        "        else:\n",
        "            print(f\"No instrument_key found for name: {name}\")\n",
        "\n",
        "    return instrument_keys\n",
        "\n",
        "instrument_keys_result = get_instrument_keys(df, names_to_search)\n",
        "# Dictionary to store DataFrames\n",
        "dfs = {}\n",
        "\n",
        "# Loop through each symbol and fetch intraday data\n",
        "for name, instrument_key in instrument_keys_result.items():\n",
        "    df_symbol = fetch_intraday_data(instrument_key)\n",
        "\n",
        "    # Convert timestamp column to datetime format\n",
        "    if 'timestamp' in df_symbol.columns:\n",
        "        df_symbol['timestamp'] = pd.to_datetime(df_symbol['timestamp'])\n",
        "\n",
        "    # Store the DataFrame in the dictionary with a name like df_reliance\n",
        "    dfs[f\"df_{name.lower()}\"] = df_symbol\n",
        "\n",
        "# # Access individual DataFrames using dfs dictionary\n",
        "# df_nse_eq = dfs.get('df_nse_eq%7cine848e01016', pd.DataFrame())\n",
        "\n",
        "\n",
        "# Function to clean the data for a given DataFrame\n",
        "def clean_data(df):\n",
        "    # Create an empty DataFrame with the desired column names\n",
        "    columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'open interest']\n",
        "    df_cleaned_data = pd.DataFrame(columns=columns)\n",
        "\n",
        "    # Iterate through each row in the original DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        # Extract data from the list\n",
        "        row_data = row[0] if isinstance(row[0], list) else row[0][0]  # Assuming each row contains a list or list within a list\n",
        "\n",
        "        # Reshape the row_data if needed\n",
        "        if isinstance(row_data, list) and len(row_data) == 1:\n",
        "            row_data = row_data[0]\n",
        "\n",
        "        # Create a temporary DataFrame\n",
        "        temp_df = pd.DataFrame([row_data], columns=columns[:len(row_data)])\n",
        "\n",
        "        # Append the temporary DataFrame to the cleaned DataFrame\n",
        "        df_cleaned_data = pd.concat([df_cleaned_data, temp_df], ignore_index=True)\n",
        "\n",
        "    # Convert timestamp column to datetime format\n",
        "    df_cleaned_data['timestamp'] = pd.to_datetime(df_cleaned_data['timestamp'])\n",
        "\n",
        "    return df_cleaned_data\n",
        "for name, df_symbol in dfs.items():\n",
        "    cleaned_df = clean_data(df_symbol)\n",
        "    dfs[name] = cleaned_df\n",
        "\n",
        "# Access and print individual cleaned DataFrames\n",
        "for name, cleaned_df in dfs.items():\n",
        "    cleaned_df['timestamp'] = cleaned_df['timestamp'].astype(str).str[:-6]\n",
        "    print(f\"Cleaned DataFrame for {name}:\")\n",
        "    print(cleaned_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SucPtPmeNU9-",
        "outputId": "2f46eb8e-5ad6-45b7-d224-563b28bcdcb3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned DataFrame for df_360one:\n",
            "Empty DataFrame\n",
            "Columns: [timestamp, open, high, low, close, volume, open interest]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfjijm5rePOy",
        "outputId": "6ab46aa1-2f13-484c-db07-15efc0df5dbb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "repo_path = \"LAXMINARAYAN-SHARMA/INTRADAY_DATA\"\n",
        "github_token = \"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\"\n",
        "def read_and_append_files(dfs):\n",
        "    # Iterate over each row in the DataFrame\n",
        "    # for index, row in df.iterrows():\n",
        "    for name, cleaned_df in dfs.items():\n",
        "        # print(cleaned_df)\n",
        "\n",
        "        # Extract the name from the specific column\n",
        "        # name = row['Symbol']\n",
        "        # print(name)\n",
        "        # Construct the file name\n",
        "        file_name = f\"{name}_historical_1minute_data.xlsx\"\n",
        "\n",
        "        # GitHub URL where the file is stored\n",
        "        github_url = f\"https://raw.githubusercontent.com/LAXMINARAYAN-SHARMA/INTRADAY_DATA/main/{file_name}\"\n",
        "\n",
        "        # Fetch the file content from GitHub\n",
        "        response = requests.get(github_url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            # Read the Excel file\n",
        "            excel_data = response.content\n",
        "            df_to_append = pd.read_excel(BytesIO(excel_data))\n",
        "\n",
        "            # Append the DataFrame\n",
        "            dfs[name] = cleaned_df.append(df_to_append, ignore_index=True)\n",
        "        else:\n",
        "            print(f\"Failed to retrieve file: {file_name}\")\n",
        "\n",
        "    return dfs\n",
        "\n",
        "# Assuming you have a DataFrame named 'complete_df'\n",
        "# containing the column 'name'\n",
        "complete_df = read_and_append_files(dfs)\n",
        "for name, cleaned_df in complete_df.items():\n",
        "    print(f\"Cleaned DataFrame for {name}:\")\n",
        "    print(cleaned_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8_09FjONrUA",
        "outputId": "b544e73f-c0a1-458e-c778-18c1a76b36d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned DataFrame for df_360one:\n",
            "                timestamp    open    high     low   close volume open interest\n",
            "0     2024-02-09 15:29:00  645.10  651.05  644.00  644.65   7340             0\n",
            "1     2024-02-09 15:28:00  651.10  654.15  645.05  645.05   6301             0\n",
            "2     2024-02-09 15:27:00  654.95  655.85  650.35  650.35    767             0\n",
            "3     2024-02-09 15:26:00  655.85  655.85  653.20  654.90   3875             0\n",
            "4     2024-02-09 15:25:00  655.50  656.15  655.00  655.50   2001             0\n",
            "...                   ...     ...     ...     ...     ...    ...           ...\n",
            "47675 2023-08-07 09:19:00  509.70  509.85  509.20  509.45    287             0\n",
            "47676 2023-08-07 09:18:00  510.25  510.60  510.25  510.55    300             0\n",
            "47677 2023-08-07 09:17:00  510.80  510.80  510.05  510.70    506             0\n",
            "47678 2023-08-07 09:16:00  512.75  512.75  510.10  510.10   1603             0\n",
            "47679 2023-08-07 09:15:00  508.95  512.85  508.95  511.50   1002             0\n",
            "\n",
            "[47680 rows x 7 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-9fc769e8dd26>:31: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  dfs[name] = cleaned_df.append(df_to_append, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lYGXcZmMQnp",
        "outputId": "04c5ecd8-98b9-4887-f3f8-3b522cceec11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pygithub in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from Pygithub) (1.5.0)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from Pygithub) (2.31.0)\n",
            "Requirement already satisfied: pyjwt[crypto]>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from Pygithub) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from Pygithub) (4.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from Pygithub) (2.0.7)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from Pygithub) (1.2.14)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->Pygithub) (42.0.2)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->Pygithub) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->Pygithub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->Pygithub) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->Pygithub) (2024.2.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->Pygithub) (1.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->Pygithub) (2.21)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "DataFrame 'df_360one' successfully saved to Excel file: /content/df_360one_historical_1minute_data.xlsx\n",
            "File 'df_360one_historical_1minute_data.xlsx' updated on GitHub.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # Example usage for each DataFrame\n",
        "# df_nse_eq_cleaned = clean_data(df_nse_eq)\n",
        "# df_other_symbol_cleaned = clean_data(df_other_symbol)\n",
        "\n",
        "# # Print cleaned DataFrames\n",
        "# print(\"Cleaned DataFrame for NSE_EQ%7CINE848E01016:\")\n",
        "# print(df_nse_eq_cleaned)\n",
        "\n",
        "# print(\"\\nCleaned DataFrame for OTHER_SYMBOL:\")\n",
        "# print(df_other_symbol_cleaned)\n",
        "\n",
        "\n",
        "!pip install Pygithub\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from github import Github\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to save DataFrames to Excel files in a loop\n",
        "def save_all_to_excel(dfs, repo_path, github_token):\n",
        "    # Initialize the GitHub client\n",
        "    g = Github(github_token)\n",
        "    repo = g.get_repo(repo_path)\n",
        "\n",
        "    for df_name, df in dfs.items():\n",
        "        # Remove timezone information from datetime columns\n",
        "\n",
        "        file_name = f\"{df_name}_historical_1minute_data.xlsx\"\n",
        "        file_path = '/content/' + file_name\n",
        "        df.to_excel(file_path, index=False)\n",
        "        print(f\"DataFrame '{df_name}' successfully saved to Excel file: {file_path}\")\n",
        "\n",
        "        # Fetch the existing file from GitHub\n",
        "        try:\n",
        "            existing_file = repo.get_contents(file_name)\n",
        "            existing_file_sha = existing_file.sha\n",
        "        except Exception as e:\n",
        "            existing_file_sha = None\n",
        "            print(f\"Failed to fetch existing file: {str(e)}\")\n",
        "\n",
        "        # Upload the file to GitHub\n",
        "        with open(file_path, 'rb') as file:\n",
        "            content = file.read()\n",
        "            if existing_file_sha:\n",
        "                repo.update_file(file_name, f\"Update {file_name}\", content, existing_file_sha)\n",
        "                print(f\"File '{file_name}' updated on GitHub.\")\n",
        "            else:\n",
        "                repo.create_file(file_name, f\"Add {file_name}\", content)\n",
        "                print(f\"File '{file_name}' created on GitHub.\")\n",
        "\n",
        "\n",
        "# Specify your GitHub repository path and personal access token\n",
        "repo_path = \"LAXMINARAYAN-SHARMA/INTRADAY_DATA\"\n",
        "github_token = \"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\"\n",
        "\n",
        "# Save all DataFrames to individual Excel files and upload them to GitHub\n",
        "save_all_to_excel(dfs, repo_path, github_token)\n",
        "\n"
      ]
    }
  ]
}